---
title: "Thrivikram"
output:
  word_document: default
  html_document: default
date: "2025-11-03"
---


```{r}
library(tidyverse)
library(lubridate)
library(forecast)
library(gridExtra)
library(ranger)
library(randomForest)
df <- read_csv("Airline_CS.csv", show_col_types = FALSE)
```


```{r}
str(df)
```



```{r}
sum(is.na(df$satisfaction))

```




```{r}
ggplot(df, aes(x = satisfaction)) +
  geom_bar() +
  labs(
    title = "Distribution of Customer Satisfaction",
    x = "Satisfaction",
    y = "Count"
  ) +
  theme_minimal()

```
There are more satisfied then dis satisfied travelers


```{r}
ggplot(df, aes(x = Class, fill = satisfaction)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Proportion of Satisfaction by Travel Class",
    x = "Class",
    y = "Percentage",
    fill = "Satisfaction"
  ) +
  theme_minimal()

```

Business class has the higest level of satisfaction, then followed by economy plus and then economy class


```{r}
#  Basic cleaning: drop ...23 and remove rows with any NA
df_clean <- df %>%
  select(-`...23`) %>%       # drop the all-NA column
  tidyr::drop_na()           # remove rows with missing values

#  Convert character variables to factors
df_clean <- df_clean %>%
  mutate(across(where(is.character), as.factor))

# Make sure satisfaction is a factor, set "dissatisfied" as reference
df_clean$satisfaction <- relevel(df_clean$satisfaction,
                                 ref = "dissatisfied")


```



```{r}
#  Train / validation split: 75% / 25%
set.seed(123)   # for reproducibility

n <- nrow(df_clean)
train_index <- sample(seq_len(n), size = 0.75 * n)

train_data <- df_clean[train_index, ]
valid_data <- df_clean[-train_index, ]

#  Fit multiple logistic regression (all other variables as predictors)
logit_model <- glm(
  satisfaction ~ .,
  data   = train_data,
  family = binomial
)

summary(logit_model)   # see model output

#  Predict on validation set
valid_data$pred_prob <- predict(
  logit_model,
  newdata = valid_data,
  type = "response"
)

# Classify using 0.5 cutoff
valid_data$pred_class <- ifelse(valid_data$pred_prob >= 0.5,
                                "satisfied", "dissatisfied") %>%
  factor(levels = levels(df_clean$satisfaction))

#  Confusion matrix & accuracy on validation set
table(Predicted = valid_data$pred_class,
      Actual    = valid_data$satisfaction)

mean(valid_data$pred_class == valid_data$satisfaction)
```

Interpretation of key coefficients

1. A loyal customer has much higher odds of being satisfied compared with a non-loyal customer (large positive coefficient for customer type - Loyal → odds of satisfaction are several times larger.

2. Business class passengers are more satisfied than Eco and Eco Plus passengers- as shown by the negative coefficients for Eco and Eco Plus versus Business. With higher ratings for seat comfort, inflight entertainment, on-board service, leg room, check-in service, cleanliness, baggage handling, online support, ease of online booking, online boarding, and gate location, the probability of being satisfied is increased.

3. Larger flight distance and greater arrival delays decrease satisfaction-thus, the coefficients are negative-meaning that longer flights and late arrivals are associated with lower satisfaction.

4. The negative signs for convenience of departure/arrival time, food and drink, and in-flight will probably reﬂect overlap with other service variables (multicollinearity) rather than truly “worse when higher”.

Significance of predictors

1. All the predictors have very large |z-scores| and p-values < 0.01, most < 2e-16, so all variables in the model are statistically significant.

2. In this model, the variables with the largest |z-scores| correspond to customer type (loyal), inflight entertainment, seat comfort, leg room, on-board service, and check-in service, which are, therefore, the most influential drivers of customer satisfaction.


confusion matrix  and accuracy 

1. The logistic regression model has an accuracy of about 82.8%, which means it correctly classifies satisfaction for roughly 83 out of 100 customers in the validation set.

2. The confusion matrix from the above report indicates accurate predictions of 11,991 dissatisfied and 14,902 satisfied customers, while 2,760 were false positives, indicating those who were predicted to be satisfied but actually dissatisfied, and 2,817 were false negatives, indicating those predicted to be dissatisfied but actually satisfied.

3. Given that the number of false positives and false negatives is fairly well-balanced, with an overall high accuracy, the model would be a good fit for the prediction of customer satisfaction, though still leaving much room for improvement by reducing mis classifications in both groups.


```{r}
## 1) Clean & prepare data ----
df_rf <- df %>%
  select(-`...23`) %>%      # drop empty column
  drop_na()                 # remove rows with NAs

```



```{r}
df_rf <- as.data.frame(df_rf)
names(df_rf) <- make.names(names(df_rf))   # fix spaces in names
```


```{r}
df_rf$satisfaction <- as.factor(df_rf$satisfaction)

## 2) Train / validation split (75 / 25) ----
set.seed(123)

n <- nrow(df_rf)
train_index <- sample(seq_len(n), size = 0.75 * n)

train_data <- df_rf[train_index, ]
valid_data <- df_rf[-train_index, ]
```


```{r}
## 3) Fit Random Forest with ranger (much faster) ----
rf_model <- ranger(
  formula      = satisfaction ~ .,
  data         = train_data,
  num.trees    = 150,              # fewer trees = faster
  mtry         = floor(sqrt(ncol(train_data) - 1)),
  importance   = "impurity",
  classification = TRUE
)
```
```{r}
## 4) Predict on validation set ----
rf_pred <- predict(rf_model, data = valid_data)$predictions

```


```{r}
valid_data$rf_pred <- rf_pred

## 5) Confusion matrix ----
table(Predicted = valid_data$rf_pred,
      Actual    = valid_data$satisfaction)
```


```{r}
## 6) Accuracy ----
mean(valid_data$rf_pred == valid_data$satisfaction)

## 7) Variable importance plot (simple) ----
imp <- rf_model$variable.importance
imp <- sort(imp, decreasing = TRUE)

barplot(
  imp,
  las = 2,
  cex.names = 0.7,
  main = "Random Forest Variable Importance (ranger)",
  ylab = "Impurity Importance"
)
```
The significance of the predictors based on Mean Decrease Accuracy and Mean

The Random Forest model achieved a very high accuracy of 95.65%, which is a significant improvement over the logistic regression accuracy of about 82.8%. This means the model is predicting customer satisfaction much more accurately using nonlinear relationships and interactions.

From the variable importance plot, Inflight_entertainment is clearly the most influential predictor, followed by Seat_comfort, Ease_of_Online_booking, Online_support, and On_board_service. These features contribute the most to improving predictions and reducing node impurity across the trees.

The next set of predictors—such as Food_drink, Customer_Type (loyal), Leg_room, Online_boarding, Flight_Distance, and Class—also play an important role but with smaller importance values compared to the top group.

Variables toward the right side of the graph, including Departure_Arrival_time_convenient, Cleanliness, Baggage_handling, Gate_location, Type_of_Travel, Inflight_wifi, and both Arrival_Delay and Departure_Delay, show lower importance, meaning they have only a minor impact on improving prediction accuracy in this model.


The decision trees model fit using the confusion matrix and accuracy.
 
The Random Forest model achieves an accuracy of about 95.7%, meaning it correctly predicts satisfaction for almost 96 out of 100 customers in the validation set.

From the confusion matrix, it correctly classifies 14,219 dissatisfied and 16,839 satisfied customers, with only 532 false positives (predicted satisfied but actually dissatisfied) and 880 false negatives (predicted dissatisfied but actually satisfied).

Misclassifications are relatively low in both groups, and the high overall accuracy indicates that the decision trees (Random Forest) model provides an excellent fit for predicting customer satisfaction on this dataset.


Logistic Regression vs. Random Forest

The logistic regression model achieved an accuracy of about 82.8% on the validation set. It misclassified a noticeable number of cases in both classes (2,760 false positives and 2,817 false negatives), which indicates that a simple linear boundary in the predictor space is not capturing all the structure in the data. Its main advantage is interpretability: the coefficients clearly show how each predictor (e.g., class, loyalty, seat comfort, inflight entertainment) affects the odds of being satisfied.

The Random Forest (decision trees) model performed substantially better, with a validation accuracy of about 95.7%. It correctly classified most customers, with only 532 false positives and 880 false negatives, showing a much lower error rate for both satisfied and dissatisfied groups. The variable importance results also highlight the dominant role of inflight entertainment, seat comfort, online booking, online support, and on-board service in predicting satisfaction.

Overall, the Random Forest model performs better than logistic regression in terms of predictive accuracy and misclassification rates. Logistic regression remains useful for explaining the direction and size of effects, but Random Forest is more effective for accurate prediction in this dataset because it can capture nonlinear relationships and complex interactions among the service-quality variables.

